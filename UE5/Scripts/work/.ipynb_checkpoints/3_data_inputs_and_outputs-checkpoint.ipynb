{"cells":[{"cell_type":"markdown","source":["# Reading and Writing Data with Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"37d500b1-dca6-4295-9133-3bdca61c876b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import pyspark\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"026bed82-44b1-451c-844e-c909f69e52c5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Since we're using Spark locally we already have both a sparkcontext and a sparksession running. We can update some of the parameters, such our application's name. Let's just call it \"Our first Python Spark SQL example\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dc02e63a-976a-400f-a7a9-bf34d2258ad5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark = SparkSession.builder.getOrCreate()\n#spark = pyspark.SparkContext.getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2913b186-4398-4314-8943-771c9d4aef4a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's check if the change went through"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0fca53e3-838f-4f8b-aac6-04dc7c475b98","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.sparkContext.getConf().getAll()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2ffe9d0f-fb5a-4afd-8e85-34de7a109cef","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[3]: [('spark.databricks.preemption.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.clusterFirstOnDemand', '1'),\n ('spark.sql.hive.metastore.jars', '/databricks/databricks-hive/*'),\n ('spark.driver.tempDirectory', '/local_disk0/tmp'),\n ('spark.sql.warehouse.dir', 'dbfs:/user/hive/warehouse'),\n ('spark.databricks.managedCatalog.clientClassName',\n  'com.databricks.managedcatalog.ManagedCatalogClientImpl'),\n ('spark.databricks.credential.scope.fs.gs.auth.access.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.CredentialScopeGCPTokenProvider'),\n ('spark.hadoop.fs.fcfs-s3.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.s3a.retry.limit', '20'),\n ('spark.sql.streaming.checkpointFileManagerClass',\n  'com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager'),\n ('spark.databricks.service.dbutils.repl.backend',\n  'com.databricks.dbconnect.ReplDBUtils'),\n ('spark.databricks.clusterUsageTags.driverInstancePrivateIp', '10.139.0.4'),\n ('spark.hadoop.databricks.s3.verifyBucketExists.enabled', 'false'),\n ('spark.streaming.driver.writeAheadLog.allowBatching', 'true'),\n ('spark.databricks.clusterSource', 'UI'),\n ('spark.databricks.clusterUsageTags.orgId', '4401470379569556'),\n ('spark.databricks.clusterUsageTags.clusterOwnerOrgId', '4401470379569556'),\n ('spark.hadoop.hive.server2.transport.mode', 'http'),\n ('spark.databricks.acl.dfAclsEnabled', 'false'),\n ('spark.driver.host', '10.139.64.4'),\n ('spark.databricks.clusterUsageTags.workerEnvironmentId',\n  'workerenv-4401470379569556'),\n ('spark.hadoop.fs.cpfs-adl.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.hailEnabled', 'false'),\n ('spark.hadoop.databricks.s3.amazonS3Client.cache.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled', 'false'),\n ('spark.databricks.clusterUsageTags.containerType', 'LXC'),\n ('spark.eventLog.enabled', 'false'),\n ('spark.databricks.clusterUsageTags.isIMv2Enabled', 'false'),\n ('spark.hadoop.fs.stage.impl.disable.cache', 'true'),\n ('spark.hadoop.hive.hmshandler.retry.interval', '2000'),\n ('spark.executor.tempDirectory', '/local_disk0/tmp'),\n ('spark.hadoop.fs.azure.authorization.caching.enable', 'false'),\n ('spark.hadoop.fs.fcfs-abfss.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.hadoop.mapred.output.committer.class',\n  'com.databricks.backend.daemon.data.client.DirectOutputCommitter'),\n ('spark.hadoop.hive.server2.thrift.http.port', '10000'),\n ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'),\n ('spark.databricks.clusterUsageTags.instanceWorkerEnvId',\n  'workerenv-4401470379569556'),\n ('spark.sql.allowMultipleContexts', 'false'),\n ('spark.databricks.eventLog.enabled', 'true'),\n ('spark.home', '/databricks/spark'),\n ('spark.databricks.clusterUsageTags.clusterTargetWorkers', '0'),\n ('spark.hadoop.hive.server2.idle.operation.timeout', '7200000'),\n ('spark.task.reaper.enabled', 'true'),\n ('spark.storage.memoryFraction', '0.5'),\n ('eventLog.rolloverIntervalSeconds', '900'),\n ('spark.databricks.sql.configMapperClass',\n  'com.databricks.dbsql.config.SqlConfigMapperBridge'),\n ('spark.driver.maxResultSize', '4g'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline', 'false'),\n ('spark.hadoop.fs.fcfs-s3.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.delta.multiClusterWrites.enabled', 'true'),\n ('spark.worker.cleanup.enabled', 'false'),\n ('spark.sql.legacy.createHiveTableByDefault', 'false'),\n ('spark.ui.port', '40001'),\n ('spark.repl.class.uri', 'spark://10.139.64.4:34585/classes'),\n ('spark.hadoop.fs.fcfs-s3a.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.s3a.attempts.maximum', '10'),\n ('spark.databricks.clusterUsageTags.enableCredentialPassthrough', 'false'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign', 'false'),\n ('spark.databricks.clusterUsageTags.enableJdbcAutoStart', 'true'),\n ('spark.hadoop.fs.azure.user.agent.prefix', ''),\n ('spark.hadoop.fs.s3n.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough',\n  'false'),\n ('spark.hadoop.fs.fcfs-s3n.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.hadoop.fs.s3a.retry.throttle.interval', '500ms'),\n ('spark.hadoop.fs.wasb.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.clusterLogDestination', ''),\n ('spark.databricks.wsfsPublicPreview', 'true'),\n ('spark.cleaner.referenceTracking.blocking', 'false'),\n ('spark.databricks.clusterUsageTags.clusterState', 'Pending'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes',\n  'false'),\n ('spark.databricks.tahoe.logStore.azure.class',\n  'com.databricks.tahoe.store.AzureLogStore'),\n ('spark.hadoop.fs.azure.skip.metrics', 'true'),\n ('spark.hadoop.hive.hmshandler.retry.attempts', '10'),\n ('spark.scheduler.mode', 'FAIR'),\n ('spark.r.sql.derby.temp.dir', '/tmp/RtmplkRvkj'),\n ('spark.sql.sources.default', 'delta'),\n ('spark.databricks.unityCatalog.credentialManager.tokenRefreshEnabled',\n  'true'),\n ('spark.hadoop.fs.cpfs-s3n.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterWorkers', '0'),\n ('spark.databricks.clusterUsageTags.managedResourceGroup',\n  'databricks-rg-bricks-b6lhvoytb2hdi'),\n ('spark.hadoop.fs.cpfs-adl.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterId', '1207-020733-iiri6xet'),\n ('spark.hadoop.fs.fcfs-s3n.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.cpfs-abfss.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.master', 'local[*, 4]'),\n ('spark.databricks.clusterUsageTags.clusterNumCustomTags', '1'),\n ('spark.databricks.passthrough.oauth.refresher.impl',\n  'com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient'),\n ('spark.databricks.workerNodeTypeId', 'Standard_F4'),\n ('spark.sql.hive.metastore.sharedPrefixes',\n  'org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks'),\n ('spark.databricks.io.directoryCommit.enableLogicalDelete', 'false'),\n ('spark.task.reaper.killTimeout', '60s'),\n ('spark.hadoop.parquet.block.size.row.check.min', '10'),\n ('spark.hadoop.hive.server2.use.SSL', 'true'),\n ('spark.hadoop.fs.mcfs-s3a.impl',\n  'com.databricks.sql.acl.fs.ManagedCatalogFileSystem'),\n ('spark.hadoop.databricks.dbfs.client.version', 'v2'),\n ('spark.databricks.clusterUsageTags.clusterNodeType', 'Standard_F4'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb', '0'),\n ('spark.hadoop.hive.server2.keystore.path',\n  '/databricks/keys/jetty-ssl-driver-keystore.jks'),\n ('spark.databricks.credential.redactor',\n  'com.databricks.logging.secrets.CredentialRedactorProxyImpl'),\n ('spark.databricks.clusterUsageTags.clusterPinned', 'false'),\n ('spark.databricks.acl.provider',\n  'com.databricks.sql.acl.ReflectionBackedAclProvider'),\n ('spark.extraListeners',\n  'com.databricks.backend.daemon.driver.DBCEventLoggingListener'),\n ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled',\n  'false'),\n ('spark.sql.parquet.cacheMetadata', 'true'),\n ('spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2', '0'),\n ('spark.hadoop.parquet.abfs.readahead.optimization.enabled', 'true'),\n ('spark.hadoop.fs.adl.impl', 'com.databricks.adl.AdlFileSystem'),\n ('spark.hadoop.fs.cpfs-abfss.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.enableLocalDiskEncryption', 'false'),\n ('spark.databricks.tahoe.logStore.class',\n  'com.databricks.tahoe.store.DelegatingLogStore'),\n ('spark.hadoop.fs.s3.impl.disable.cache', 'true'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.db.ttl-mins', '30'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.table.ttl-mins', '30'),\n ('libraryDownload.sleepIntervalSeconds', '5'),\n ('spark.sql.hive.convertMetastoreParquet', 'true'),\n ('spark.databricks.service.dbutils.server.backend',\n  'com.databricks.dbconnect.SparkServerDBUtils'),\n ('spark.executor.id', 'driver'),\n ('spark.databricks.repl.enableClassFileCleanup', 'true'),\n ('spark.databricks.clusterUsageTags.region', 'westeurope'),\n ('spark.hadoop.fs.s3a.multipart.size', '10485760'),\n ('spark.databricks.clusterUsageTags.effectiveSparkVersion',\n  '11.3.x-scala2.12'),\n ('spark.metrics.conf', '/databricks/spark/conf/metrics.properties'),\n ('spark.databricks.clusterUsageTags.autoTerminationMinutes', '15'),\n ('spark.akka.frameSize', '256'),\n ('spark.hadoop.fs.s3a.fast.upload', 'true'),\n ('spark.databricks.clusterUsageTags.driverNodeType', 'Standard_F4'),\n ('spark.hadoop.fs.wasbs.impl',\n  'shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem'),\n ('spark.sql.streaming.stopTimeout', '15s'),\n ('spark.hadoop.hive.server2.keystore.password', '[REDACTED]'),\n ('spark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting',\n  'false'),\n ('spark.databricks.clusterUsageTags.clusterOwnerUserId', '871826826438401'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape', 'false'),\n ('spark.databricks.overrideDefaultCommitProtocol',\n  'org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol'),\n ('spark.databricks.clusterUsageTags.sparkMasterUrlType', 'Local'),\n ('spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass',\n  'com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient'),\n ('spark.databricks.clusterUsageTags.clusterNoDriverDaemon', 'false'),\n ('libraryDownload.timeoutSeconds', '180'),\n ('spark.databricks.cluster.profile', 'singleNode'),\n ('spark.hadoop.parquet.memory.pool.ratio', '0.5'),\n ('spark.databricks.clusterUsageTags.clusterScalingType', 'fixed_size'),\n ('spark.databricks.passthrough.adls.gen2.tokenProviderClassName',\n  'com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider'),\n ('spark.hadoop.fs.s3a.block.size', '67108864'),\n ('spark.databricks.tahoe.logStore.gcp.class',\n  'com.databricks.tahoe.store.GCPLogStore'),\n ('spark.serializer.objectStreamReset', '100'),\n ('spark.sql.sources.commitProtocolClass',\n  'com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol'),\n ('spark.databricks.passthrough.enabled', 'false'),\n ('spark.hadoop.fs.fcfs-s3a.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.clusterUsageTags.attribute_tag_budget', ''),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType',\n  'azure_disk_volume_type: PREMIUM_LRS\\n'),\n ('spark.databricks.clusterUsageTags.clusterPythonVersion', '3'),\n ('spark.databricks.workspaceUrl',\n  'adb-4401470379569556.16.azuredatabricks.net'),\n ('spark.databricks.clusterUsageTags.enableDfAcls', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount', '0'),\n ('spark.shuffle.service.enabled', 'true'),\n ('spark.hadoop.fs.file.impl',\n  'com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem'),\n ('spark.hadoop.fs.fcfs-wasb.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.driverInstanceId',\n  '14b48ff6ef104b7b940c3befa5f9350f'),\n ('spark.hadoop.fs.cpfs-s3.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer', ''),\n ('spark.hadoop.fs.s3a.multipart.threshold', '104857600'),\n ('spark.rpc.message.maxSize', '256'),\n ('spark.databricks.clusterUsageTags.clusterAvailability', 'ON_DEMAND_AZURE'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_suite', ''),\n ('spark.hadoop.fs.fcfs-wasbs.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.driverNfs.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.clusterMetastoreAccessType',\n  'RDS_DIRECT'),\n ('spark.databricks.clusterUsageTags.ngrokNpipEnabled', 'false'),\n ('spark.hadoop.parquet.page.metadata.validation.enabled', 'true'),\n ('spark.databricks.acl.enabled', 'false'),\n ('spark.databricks.unityCatalog.credentialManager.apiTokenProviderClassName',\n  'com.databricks.unity.TokenServiceApiTokenProvider'),\n ('spark.databricks.passthrough.glue.executorServiceFactoryClassName',\n  'com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory'),\n ('spark.databricks.clusterUsageTags.enableElasticDisk', 'true'),\n ('spark.hadoop.fs.gs.impl',\n  'shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemHadoop3'),\n ('spark.databricks.acl.scim.client',\n  'com.databricks.spark.sql.acl.client.DriverToWebappScimClient'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick', 'false'),\n ('spark.databricks.clusterUsageTags.isSingleUserCluster', 'true'),\n ('spark.hadoop.fs.adl.impl.disable.cache', 'true'),\n ('spark.hadoop.parquet.block.size.row.check.max', '10'),\n ('spark.hadoop.fs.s3a.connection.maximum', '200'),\n ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2', '0'),\n ('spark.hadoop.fs.s3a.assumed.role.credentials.provider',\n  'com.amazonaws.auth.InstanceProfileCredentialsProvider'),\n ('spark.hadoop.fs.s3a.fast.upload.active.blocks', '32'),\n ('spark.shuffle.reduceLocality.enabled', 'false'),\n ('spark.hadoop.spark.sql.sources.outputCommitterClass',\n  'com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter'),\n ('spark.hadoop.fs.fcfs-abfs.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.hadoop.fs.fcfs-abfss.impl.disable.cache', 'true'),\n ('spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled', 'false'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.table.size', '1000'),\n ('spark.sql.parquet.compression.codec', 'snappy'),\n ('spark.hadoop.fs.stage.impl',\n  'com.databricks.backend.daemon.driver.managedcatalog.PersonalStagingFileSystem'),\n ('spark.databricks.cloudProvider', 'Azure'),\n ('spark.databricks.credential.scope.fs.s3a.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.CredentialScopeS3TokenProvider'),\n ('spark.executor.memory', '3157m'),\n ('spark.databricks.cloudfetch.hasRegionSupport', 'true'),\n ('spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories',\n  'false'),\n ('spark.hadoop.fs.wasb.impl',\n  'shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem'),\n ('spark.hadoop.fs.s3a.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystemHadoop3'),\n ('spark.databricks.clusterUsageTags.sparkVersion', '11.3.x-scala2.12'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.db.size', '1000'),\n ('spark.databricks.unityCatalog.enabled', 'false'),\n ('spark.hadoop.fs.abfs.impl',\n  'shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemHadoop3'),\n ('spark.databricks.passthrough.glue.credentialsProviderFactoryClassName',\n  'com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory'),\n ('spark.app.startTime', '1670379095212'),\n ('spark.databricks.clusterUsageTags.driverContainerPrivateIp', '10.139.64.4'),\n ('spark.sparklyr-backend.threads', '1'),\n ('spark.databricks.clusterUsageTags.clusterSpotBidMaxPrice', '-1.0'),\n ('spark.hadoop.fs.fcfs-wasb.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.passthrough.s3a.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider'),\n ('spark.databricks.session.share', 'false'),\n ('spark.hadoop.fs.s3n.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystemHadoop3'),\n ('spark.hadoop.fs.idbfs.impl', 'com.databricks.io.idbfs.IdbfsFileSystem'),\n ('spark.driver.extraJavaOptions',\n  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n ('spark.hadoop.fs.dbfs.impl',\n  'com.databricks.backend.daemon.data.client.DbfsHadoop3'),\n ('spark.databricks.clusterUsageTags.clusterSku', 'STANDARD_SKU'),\n ('spark.databricks.sparkContextId', '6092437230899731836'),\n ('spark.hadoop.fs.gs.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.s3.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystemHadoop3'),\n ('spark.driver.port', '34585'),\n ('spark.databricks.driverNodeTypeId', 'Standard_F4'),\n ('spark.delta.sharing.profile.provider.class',\n  'io.delta.sharing.DeltaSharingCredentialsProvider'),\n ('spark.executor.extraJavaOptions',\n  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1'),\n ('spark.worker.aioaLazyConfig.iamReadinessCheckClientClass',\n  'com.databricks.backend.daemon.driver.NephosIamRoleCheckClient'),\n ('spark.databricks.automl.serviceEnabled', 'true'),\n ('spark.hadoop.parquet.page.size.check.estimate', 'false'),\n ('spark.databricks.clusterUsageTags.attribute_tag_service', ''),\n ('spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class',\n  'com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory'),\n ('spark.databricks.delta.preview.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.clusterResourceClass', 'SingleNode'),\n ('spark.databricks.metrics.filesystem_io_metrics', 'true'),\n ('spark.databricks.clusterUsageTags.dataPlaneRegion', 'westeurope'),\n ('spark.databricks.cloudfetch.requesterClassName',\n  'com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester'),\n ('spark.databricks.delta.logStore.crossCloud.fatal', 'true'),\n ('spark.databricks.driverNfs.clusterWidePythonLibsEnabled', 'true'),\n ('spark.files.fetchFailure.unRegisterOutputOnHost', 'true'),\n ('spark.databricks.clusterUsageTags.enableSqlAclsOnly', 'false'),\n ('spark.databricks.clusterUsageTags.clusterNumSshKeys', '0'),\n ('spark.databricks.clusterUsageTags.clusterSizeType', 'VM_CONTAINER'),\n ('spark.hadoop.databricks.fs.perfMetrics.enable', 'true'),\n ('spark.hadoop.fs.gs.outputstream.upload.chunk.size', '16777216'),\n ('spark.speculation.quantile', '0.9'),\n ('spark.databricks.clusterUsageTags.privateLinkEnabled', 'false'),\n ('spark.shuffle.manager', 'SORT'),\n ('spark.files.overwrite', 'true'),\n ('spark.databricks.credential.aws.secretKey.redactor',\n  'com.databricks.spark.util.AWSSecretKeyRedactorProxy'),\n ('spark.hadoop.fs.s3a.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes',\n  'false'),\n ('spark.r.numRBackendThreads', '1'),\n ('spark.hadoop.fs.wasbs.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.abfss.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.azure.cache.invalidator.type',\n  'com.databricks.encryption.utils.CacheInvalidatorImpl'),\n ('spark.sql.hive.metastore.version', '0.13.0'),\n ('spark.shuffle.service.port', '4048'),\n ('spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType', 'default'),\n ('spark.databricks.acl.client',\n  'com.databricks.spark.sql.acl.client.SparkSqlAclClient'),\n ('spark.streaming.driver.writeAheadLog.closeFileAfterWrite', 'true'),\n ('spark.hadoop.hive.warehouse.subdir.inherit.perms', 'false'),\n ('spark.databricks.clusterUsageTags.runtimeEngine', 'STANDARD'),\n ('spark.databricks.clusterUsageTags.isServicePrincipalCluster', 'false'),\n ('spark.databricks.credential.scope.fs.impl',\n  'com.databricks.sql.acl.fs.CredentialScopeFileSystem'),\n ('spark.databricks.clusterUsageTags.azureSubscriptionId',\n  '5dac9727-e0ea-4e50-b0f3-f95753f4c0fb'),\n ('spark.hadoop.fs.fcfs-wasbs.impl.disable.cache', 'true'),\n ('spark.databricks.passthrough.adls.tokenProviderClassName',\n  'com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider'),\n ('spark.app.name', 'Databricks Shell'),\n ('spark.driver.allowMultipleContexts', 'false'),\n ('spark.hadoop.fs.AbstractFileSystem.gs.impl',\n  'shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS'),\n ('spark.databricks.secret.sparkConf.keys.toRedact', ''),\n ('spark.rdd.compress', 'true'),\n ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException',\n  'false'),\n ('spark.databricks.python.defaultPythonRepl', 'ipykernel'),\n ('spark.app.id', 'local-1670379098148'),\n ('spark.databricks.clusterUsageTags.clusterName',\n  \"d3bde3fb-687d-4f06-b8c0-5175027fbfe5 8d5163f7-62fd-4256-af8f-14e2eb3357a5's Personal Compute Cluster\"),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env', ''),\n ('spark.databricks.eventLog.dir', 'eventlogs'),\n ('spark.databricks.credential.scope.fs.adls.gen2.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.CredentialScopeADLSTokenProvider'),\n ('spark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled', 'false'),\n ('spark.databricks.driverNfs.pathSuffix', '.ephemeral_nfs'),\n ('spark.databricks.clusterUsageTags.clusterCreator', 'Webapp'),\n ('spark.speculation', 'false'),\n ('spark.hadoop.hive.server2.session.check.interval', '60000'),\n ('spark.sql.hive.convertCTAS', 'true'),\n ('spark.hadoop.fs.s3a.max.total.tasks', '1000'),\n ('spark.hadoop.spark.sql.parquet.output.committer.class',\n  'org.apache.spark.sql.parquet.DirectParquetOutputCommitter'),\n ('spark.databricks.tahoe.logStore.aws.class',\n  'com.databricks.tahoe.store.MultiClusterLogStore'),\n ('spark.hadoop.fs.s3a.fast.upload.default', 'true'),\n ('spark.databricks.clusterUsageTags.clusterGeneration', '0'),\n ('spark.hadoop.fs.mlflowdbfs.impl',\n  'com.databricks.mlflowdbfs.MlflowdbfsFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterUnityCatalogMode',\n  'LEGACY_SINGLE_USER_STANDARD'),\n ('spark.databricks.eventLog.listenerClassName',\n  'com.databricks.backend.daemon.driver.DBCEventLoggingListener'),\n ('spark.hadoop.fs.abfs.impl.disable.cache', 'true'),\n ('spark.speculation.multiplier', '3'),\n ('spark.storage.blockManagerTimeoutIntervalMs', '300000'),\n ('spark.databricks.clusterUsageTags.clusterAllTags',\n  '[{\"key\":\"ResourceClass\",\"value\":\"SingleNode\"},{\"key\":\"Vendor\",\"value\":\"Databricks\"},{\"key\":\"Creator\",\"value\":\"coding.moh@gmail.com\"},{\"key\":\"ClusterName\",\"value\":\"d3bde3fb-687d-4f06-b8c0-5175027fbfe5 8d5163f7-62fd-4256-af8f-14e2eb3357a5\\'s Personal Compute Cluster\"},{\"key\":\"ClusterId\",\"value\":\"1207-020733-iiri6xet\"},{\"key\":\"DatabricksEnvironment\",\"value\":\"workerenv-4401470379569556\"}]'),\n ('spark.repl.class.outputDir',\n  '/local_disk0/tmp/repl/spark-6092437230899731836-99809bbf-b25d-4a18-87db-b765dc044ed3'),\n ('spark.sparkr.use.daemon', 'false'),\n ('spark.scheduler.listenerbus.eventqueue.capacity', '20000'),\n ('spark.databricks.clusterUsageTags.driverPublicDns', '108.143.52.240'),\n ('spark.databricks.clusterUsageTags.clusterStateMessage', 'Starting Spark'),\n ('spark.hadoop.parquet.page.write-checksum.enabled', 'true'),\n ('spark.hadoop.databricks.s3commit.client.sslTrustAll', 'false'),\n ('spark.hadoop.fs.s3a.threads.max', '136'),\n ('spark.r.backendConnectionTimeout', '604800'),\n ('spark.hadoop.fs.abfss.impl',\n  'shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystemHadoop3'),\n ('spark.hadoop.hive.server2.idle.session.timeout', '900000'),\n ('spark.databricks.redactor',\n  'com.databricks.spark.util.DatabricksSparkLogRedactorProxy'),\n ('spark.executor.extraClassPath',\n  '/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*'),\n ('spark.databricks.clusterUsageTags.userId', '871826826438401'),\n ('spark.hadoop.fs.fcfs-abfs.impl.disable.cache', 'true'),\n ('spark.hadoop.parquet.page.verify-checksum.enabled', 'true'),\n ('spark.logConf', 'true'),\n ('spark.databricks.clusterUsageTags.enableJobsAutostart', 'true'),\n ('spark.hadoop.hive.server2.enable.doAs', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedSparkVersion',\n  '11.3.x-scala2.12'),\n ('spark.hadoop.parquet.filter.columnindex.enabled', 'false'),\n ('spark.hadoop.spark.driverproxy.customHeadersToProperties',\n  'X-Databricks-User-Token:spark.databricks.token,X-Databricks-Non-UC-User-Token:spark.databricks.non.uc.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name'),\n ('spark.shuffle.memoryFraction', '0.2'),\n ('spark.databricks.clusterUsageTags.driverContainerId',\n  'fffe1c2a84be4ed3b6e4e7e5c8acc9c6'),\n ('spark.hadoop.fs.dbfsartifacts.impl',\n  'com.databricks.backend.daemon.data.client.DBFSV1'),\n ('spark.hadoop.fs.cpfs-s3a.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.hadoop.fs.s3a.connection.timeout', '50000'),\n ('spark.databricks.secret.envVar.keys.toRedact', ''),\n ('spark.databricks.clusterUsageTags.cloudProvider', 'Azure'),\n ('spark.files.useFetchCache', 'false')]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[3]: [('spark.databricks.preemption.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.clusterFirstOnDemand', '1'),\n ('spark.sql.hive.metastore.jars', '/databricks/databricks-hive/*'),\n ('spark.driver.tempDirectory', '/local_disk0/tmp'),\n ('spark.sql.warehouse.dir', 'dbfs:/user/hive/warehouse'),\n ('spark.databricks.managedCatalog.clientClassName',\n  'com.databricks.managedcatalog.ManagedCatalogClientImpl'),\n ('spark.databricks.credential.scope.fs.gs.auth.access.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.CredentialScopeGCPTokenProvider'),\n ('spark.hadoop.fs.fcfs-s3.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.s3a.retry.limit', '20'),\n ('spark.sql.streaming.checkpointFileManagerClass',\n  'com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager'),\n ('spark.databricks.service.dbutils.repl.backend',\n  'com.databricks.dbconnect.ReplDBUtils'),\n ('spark.databricks.clusterUsageTags.driverInstancePrivateIp', '10.139.0.4'),\n ('spark.hadoop.databricks.s3.verifyBucketExists.enabled', 'false'),\n ('spark.streaming.driver.writeAheadLog.allowBatching', 'true'),\n ('spark.databricks.clusterSource', 'UI'),\n ('spark.databricks.clusterUsageTags.orgId', '4401470379569556'),\n ('spark.databricks.clusterUsageTags.clusterOwnerOrgId', '4401470379569556'),\n ('spark.hadoop.hive.server2.transport.mode', 'http'),\n ('spark.databricks.acl.dfAclsEnabled', 'false'),\n ('spark.driver.host', '10.139.64.4'),\n ('spark.databricks.clusterUsageTags.workerEnvironmentId',\n  'workerenv-4401470379569556'),\n ('spark.hadoop.fs.cpfs-adl.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.hailEnabled', 'false'),\n ('spark.hadoop.databricks.s3.amazonS3Client.cache.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled', 'false'),\n ('spark.databricks.clusterUsageTags.containerType', 'LXC'),\n ('spark.eventLog.enabled', 'false'),\n ('spark.databricks.clusterUsageTags.isIMv2Enabled', 'false'),\n ('spark.hadoop.fs.stage.impl.disable.cache', 'true'),\n ('spark.hadoop.hive.hmshandler.retry.interval', '2000'),\n ('spark.executor.tempDirectory', '/local_disk0/tmp'),\n ('spark.hadoop.fs.azure.authorization.caching.enable', 'false'),\n ('spark.hadoop.fs.fcfs-abfss.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.hadoop.mapred.output.committer.class',\n  'com.databricks.backend.daemon.data.client.DirectOutputCommitter'),\n ('spark.hadoop.hive.server2.thrift.http.port', '10000'),\n ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'),\n ('spark.databricks.clusterUsageTags.instanceWorkerEnvId',\n  'workerenv-4401470379569556'),\n ('spark.sql.allowMultipleContexts', 'false'),\n ('spark.databricks.eventLog.enabled', 'true'),\n ('spark.home', '/databricks/spark'),\n ('spark.databricks.clusterUsageTags.clusterTargetWorkers', '0'),\n ('spark.hadoop.hive.server2.idle.operation.timeout', '7200000'),\n ('spark.task.reaper.enabled', 'true'),\n ('spark.storage.memoryFraction', '0.5'),\n ('eventLog.rolloverIntervalSeconds', '900'),\n ('spark.databricks.sql.configMapperClass',\n  'com.databricks.dbsql.config.SqlConfigMapperBridge'),\n ('spark.driver.maxResultSize', '4g'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline', 'false'),\n ('spark.hadoop.fs.fcfs-s3.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.delta.multiClusterWrites.enabled', 'true'),\n ('spark.worker.cleanup.enabled', 'false'),\n ('spark.sql.legacy.createHiveTableByDefault', 'false'),\n ('spark.ui.port', '40001'),\n ('spark.repl.class.uri', 'spark://10.139.64.4:34585/classes'),\n ('spark.hadoop.fs.fcfs-s3a.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.s3a.attempts.maximum', '10'),\n ('spark.databricks.clusterUsageTags.enableCredentialPassthrough', 'false'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign', 'false'),\n ('spark.databricks.clusterUsageTags.enableJdbcAutoStart', 'true'),\n ('spark.hadoop.fs.azure.user.agent.prefix', ''),\n ('spark.hadoop.fs.s3n.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough',\n  'false'),\n ('spark.hadoop.fs.fcfs-s3n.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.hadoop.fs.s3a.retry.throttle.interval', '500ms'),\n ('spark.hadoop.fs.wasb.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.clusterLogDestination', ''),\n ('spark.databricks.wsfsPublicPreview', 'true'),\n ('spark.cleaner.referenceTracking.blocking', 'false'),\n ('spark.databricks.clusterUsageTags.clusterState', 'Pending'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes',\n  'false'),\n ('spark.databricks.tahoe.logStore.azure.class',\n  'com.databricks.tahoe.store.AzureLogStore'),\n ('spark.hadoop.fs.azure.skip.metrics', 'true'),\n ('spark.hadoop.hive.hmshandler.retry.attempts', '10'),\n ('spark.scheduler.mode', 'FAIR'),\n ('spark.r.sql.derby.temp.dir', '/tmp/RtmplkRvkj'),\n ('spark.sql.sources.default', 'delta'),\n ('spark.databricks.unityCatalog.credentialManager.tokenRefreshEnabled',\n  'true'),\n ('spark.hadoop.fs.cpfs-s3n.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterWorkers', '0'),\n ('spark.databricks.clusterUsageTags.managedResourceGroup',\n  'databricks-rg-bricks-b6lhvoytb2hdi'),\n ('spark.hadoop.fs.cpfs-adl.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterId', '1207-020733-iiri6xet'),\n ('spark.hadoop.fs.fcfs-s3n.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.cpfs-abfss.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.master', 'local[*, 4]'),\n ('spark.databricks.clusterUsageTags.clusterNumCustomTags', '1'),\n ('spark.databricks.passthrough.oauth.refresher.impl',\n  'com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient'),\n ('spark.databricks.workerNodeTypeId', 'Standard_F4'),\n ('spark.sql.hive.metastore.sharedPrefixes',\n  'org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks'),\n ('spark.databricks.io.directoryCommit.enableLogicalDelete', 'false'),\n ('spark.task.reaper.killTimeout', '60s'),\n ('spark.hadoop.parquet.block.size.row.check.min', '10'),\n ('spark.hadoop.hive.server2.use.SSL', 'true'),\n ('spark.hadoop.fs.mcfs-s3a.impl',\n  'com.databricks.sql.acl.fs.ManagedCatalogFileSystem'),\n ('spark.hadoop.databricks.dbfs.client.version', 'v2'),\n ('spark.databricks.clusterUsageTags.clusterNodeType', 'Standard_F4'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb', '0'),\n ('spark.hadoop.hive.server2.keystore.path',\n  '/databricks/keys/jetty-ssl-driver-keystore.jks'),\n ('spark.databricks.credential.redactor',\n  'com.databricks.logging.secrets.CredentialRedactorProxyImpl'),\n ('spark.databricks.clusterUsageTags.clusterPinned', 'false'),\n ('spark.databricks.acl.provider',\n  'com.databricks.sql.acl.ReflectionBackedAclProvider'),\n ('spark.extraListeners',\n  'com.databricks.backend.daemon.driver.DBCEventLoggingListener'),\n ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled',\n  'false'),\n ('spark.sql.parquet.cacheMetadata', 'true'),\n ('spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2', '0'),\n ('spark.hadoop.parquet.abfs.readahead.optimization.enabled', 'true'),\n ('spark.hadoop.fs.adl.impl', 'com.databricks.adl.AdlFileSystem'),\n ('spark.hadoop.fs.cpfs-abfss.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.enableLocalDiskEncryption', 'false'),\n ('spark.databricks.tahoe.logStore.class',\n  'com.databricks.tahoe.store.DelegatingLogStore'),\n ('spark.hadoop.fs.s3.impl.disable.cache', 'true'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.db.ttl-mins', '30'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.table.ttl-mins', '30'),\n ('libraryDownload.sleepIntervalSeconds', '5'),\n ('spark.sql.hive.convertMetastoreParquet', 'true'),\n ('spark.databricks.service.dbutils.server.backend',\n  'com.databricks.dbconnect.SparkServerDBUtils'),\n ('spark.executor.id', 'driver'),\n ('spark.databricks.repl.enableClassFileCleanup', 'true'),\n ('spark.databricks.clusterUsageTags.region', 'westeurope'),\n ('spark.hadoop.fs.s3a.multipart.size', '10485760'),\n ('spark.databricks.clusterUsageTags.effectiveSparkVersion',\n  '11.3.x-scala2.12'),\n ('spark.metrics.conf', '/databricks/spark/conf/metrics.properties'),\n ('spark.databricks.clusterUsageTags.autoTerminationMinutes', '15'),\n ('spark.akka.frameSize', '256'),\n ('spark.hadoop.fs.s3a.fast.upload', 'true'),\n ('spark.databricks.clusterUsageTags.driverNodeType', 'Standard_F4'),\n ('spark.hadoop.fs.wasbs.impl',\n  'shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem'),\n ('spark.sql.streaming.stopTimeout', '15s'),\n ('spark.hadoop.hive.server2.keystore.password', '[REDACTED]'),\n ('spark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting',\n  'false'),\n ('spark.databricks.clusterUsageTags.clusterOwnerUserId', '871826826438401'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape', 'false'),\n ('spark.databricks.overrideDefaultCommitProtocol',\n  'org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol'),\n ('spark.databricks.clusterUsageTags.sparkMasterUrlType', 'Local'),\n ('spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass',\n  'com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient'),\n ('spark.databricks.clusterUsageTags.clusterNoDriverDaemon', 'false'),\n ('libraryDownload.timeoutSeconds', '180'),\n ('spark.databricks.cluster.profile', 'singleNode'),\n ('spark.hadoop.parquet.memory.pool.ratio', '0.5'),\n ('spark.databricks.clusterUsageTags.clusterScalingType', 'fixed_size'),\n ('spark.databricks.passthrough.adls.gen2.tokenProviderClassName',\n  'com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider'),\n ('spark.hadoop.fs.s3a.block.size', '67108864'),\n ('spark.databricks.tahoe.logStore.gcp.class',\n  'com.databricks.tahoe.store.GCPLogStore'),\n ('spark.serializer.objectStreamReset', '100'),\n ('spark.sql.sources.commitProtocolClass',\n  'com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol'),\n ('spark.databricks.passthrough.enabled', 'false'),\n ('spark.hadoop.fs.fcfs-s3a.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.clusterUsageTags.attribute_tag_budget', ''),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType',\n  'azure_disk_volume_type: PREMIUM_LRS\\n'),\n ('spark.databricks.clusterUsageTags.clusterPythonVersion', '3'),\n ('spark.databricks.workspaceUrl',\n  'adb-4401470379569556.16.azuredatabricks.net'),\n ('spark.databricks.clusterUsageTags.enableDfAcls', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount', '0'),\n ('spark.shuffle.service.enabled', 'true'),\n ('spark.hadoop.fs.file.impl',\n  'com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem'),\n ('spark.hadoop.fs.fcfs-wasb.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.driverInstanceId',\n  '14b48ff6ef104b7b940c3befa5f9350f'),\n ('spark.hadoop.fs.cpfs-s3.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer', ''),\n ('spark.hadoop.fs.s3a.multipart.threshold', '104857600'),\n ('spark.rpc.message.maxSize', '256'),\n ('spark.databricks.clusterUsageTags.clusterAvailability', 'ON_DEMAND_AZURE'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_suite', ''),\n ('spark.hadoop.fs.fcfs-wasbs.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.driverNfs.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.clusterMetastoreAccessType',\n  'RDS_DIRECT'),\n ('spark.databricks.clusterUsageTags.ngrokNpipEnabled', 'false'),\n ('spark.hadoop.parquet.page.metadata.validation.enabled', 'true'),\n ('spark.databricks.acl.enabled', 'false'),\n ('spark.databricks.unityCatalog.credentialManager.apiTokenProviderClassName',\n  'com.databricks.unity.TokenServiceApiTokenProvider'),\n ('spark.databricks.passthrough.glue.executorServiceFactoryClassName',\n  'com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory'),\n ('spark.databricks.clusterUsageTags.enableElasticDisk', 'true'),\n ('spark.hadoop.fs.gs.impl',\n  'shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemHadoop3'),\n ('spark.databricks.acl.scim.client',\n  'com.databricks.spark.sql.acl.client.DriverToWebappScimClient'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick', 'false'),\n ('spark.databricks.clusterUsageTags.isSingleUserCluster', 'true'),\n ('spark.hadoop.fs.adl.impl.disable.cache', 'true'),\n ('spark.hadoop.parquet.block.size.row.check.max', '10'),\n ('spark.hadoop.fs.s3a.connection.maximum', '200'),\n ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2', '0'),\n ('spark.hadoop.fs.s3a.assumed.role.credentials.provider',\n  'com.amazonaws.auth.InstanceProfileCredentialsProvider'),\n ('spark.hadoop.fs.s3a.fast.upload.active.blocks', '32'),\n ('spark.shuffle.reduceLocality.enabled', 'false'),\n ('spark.hadoop.spark.sql.sources.outputCommitterClass',\n  'com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter'),\n ('spark.hadoop.fs.fcfs-abfs.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.hadoop.fs.fcfs-abfss.impl.disable.cache', 'true'),\n ('spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled', 'false'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.table.size', '1000'),\n ('spark.sql.parquet.compression.codec', 'snappy'),\n ('spark.hadoop.fs.stage.impl',\n  'com.databricks.backend.daemon.driver.managedcatalog.PersonalStagingFileSystem'),\n ('spark.databricks.cloudProvider', 'Azure'),\n ('spark.databricks.credential.scope.fs.s3a.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.CredentialScopeS3TokenProvider'),\n ('spark.executor.memory', '3157m'),\n ('spark.databricks.cloudfetch.hasRegionSupport', 'true'),\n ('spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories',\n  'false'),\n ('spark.hadoop.fs.wasb.impl',\n  'shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem'),\n ('spark.hadoop.fs.s3a.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystemHadoop3'),\n ('spark.databricks.clusterUsageTags.sparkVersion', '11.3.x-scala2.12'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.db.size', '1000'),\n ('spark.databricks.unityCatalog.enabled', 'false'),\n ('spark.hadoop.fs.abfs.impl',\n  'shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemHadoop3'),\n ('spark.databricks.passthrough.glue.credentialsProviderFactoryClassName',\n  'com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory'),\n ('spark.app.startTime', '1670379095212'),\n ('spark.databricks.clusterUsageTags.driverContainerPrivateIp', '10.139.64.4'),\n ('spark.sparklyr-backend.threads', '1'),\n ('spark.databricks.clusterUsageTags.clusterSpotBidMaxPrice', '-1.0'),\n ('spark.hadoop.fs.fcfs-wasb.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.passthrough.s3a.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider'),\n ('spark.databricks.session.share', 'false'),\n ('spark.hadoop.fs.s3n.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystemHadoop3'),\n ('spark.hadoop.fs.idbfs.impl', 'com.databricks.io.idbfs.IdbfsFileSystem'),\n ('spark.driver.extraJavaOptions',\n  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n ('spark.hadoop.fs.dbfs.impl',\n  'com.databricks.backend.daemon.data.client.DbfsHadoop3'),\n ('spark.databricks.clusterUsageTags.clusterSku', 'STANDARD_SKU'),\n ('spark.databricks.sparkContextId', '6092437230899731836'),\n ('spark.hadoop.fs.gs.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.s3.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystemHadoop3'),\n ('spark.driver.port', '34585'),\n ('spark.databricks.driverNodeTypeId', 'Standard_F4'),\n ('spark.delta.sharing.profile.provider.class',\n  'io.delta.sharing.DeltaSharingCredentialsProvider'),\n ('spark.executor.extraJavaOptions',\n  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1'),\n ('spark.worker.aioaLazyConfig.iamReadinessCheckClientClass',\n  'com.databricks.backend.daemon.driver.NephosIamRoleCheckClient'),\n ('spark.databricks.automl.serviceEnabled', 'true'),\n ('spark.hadoop.parquet.page.size.check.estimate', 'false'),\n ('spark.databricks.clusterUsageTags.attribute_tag_service', ''),\n ('spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class',\n  'com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory'),\n ('spark.databricks.delta.preview.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.clusterResourceClass', 'SingleNode'),\n ('spark.databricks.metrics.filesystem_io_metrics', 'true'),\n ('spark.databricks.clusterUsageTags.dataPlaneRegion', 'westeurope'),\n ('spark.databricks.cloudfetch.requesterClassName',\n  'com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester'),\n ('spark.databricks.delta.logStore.crossCloud.fatal', 'true'),\n ('spark.databricks.driverNfs.clusterWidePythonLibsEnabled', 'true'),\n ('spark.files.fetchFailure.unRegisterOutputOnHost', 'true'),\n ('spark.databricks.clusterUsageTags.enableSqlAclsOnly', 'false'),\n ('spark.databricks.clusterUsageTags.clusterNumSshKeys', '0'),\n ('spark.databricks.clusterUsageTags.clusterSizeType', 'VM_CONTAINER'),\n ('spark.hadoop.databricks.fs.perfMetrics.enable', 'true'),\n ('spark.hadoop.fs.gs.outputstream.upload.chunk.size', '16777216'),\n ('spark.speculation.quantile', '0.9'),\n ('spark.databricks.clusterUsageTags.privateLinkEnabled', 'false'),\n ('spark.shuffle.manager', 'SORT'),\n ('spark.files.overwrite', 'true'),\n ('spark.databricks.credential.aws.secretKey.redactor',\n  'com.databricks.spark.util.AWSSecretKeyRedactorProxy'),\n ('spark.hadoop.fs.s3a.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes',\n  'false'),\n ('spark.r.numRBackendThreads', '1'),\n ('spark.hadoop.fs.wasbs.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.abfss.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.azure.cache.invalidator.type',\n  'com.databricks.encryption.utils.CacheInvalidatorImpl'),\n ('spark.sql.hive.metastore.version', '0.13.0'),\n ('spark.shuffle.service.port', '4048'),\n ('spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType', 'default'),\n ('spark.databricks.acl.client',\n  'com.databricks.spark.sql.acl.client.SparkSqlAclClient'),\n ('spark.streaming.driver.writeAheadLog.closeFileAfterWrite', 'true'),\n ('spark.hadoop.hive.warehouse.subdir.inherit.perms', 'false'),\n ('spark.databricks.clusterUsageTags.runtimeEngine', 'STANDARD'),\n ('spark.databricks.clusterUsageTags.isServicePrincipalCluster', 'false'),\n ('spark.databricks.credential.scope.fs.impl',\n  'com.databricks.sql.acl.fs.CredentialScopeFileSystem'),\n ('spark.databricks.clusterUsageTags.azureSubscriptionId',\n  '5dac9727-e0ea-4e50-b0f3-f95753f4c0fb'),\n ('spark.hadoop.fs.fcfs-wasbs.impl.disable.cache', 'true'),\n ('spark.databricks.passthrough.adls.tokenProviderClassName',\n  'com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider'),\n ('spark.app.name', 'Databricks Shell'),\n ('spark.driver.allowMultipleContexts', 'false'),\n ('spark.hadoop.fs.AbstractFileSystem.gs.impl',\n  'shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS'),\n ('spark.databricks.secret.sparkConf.keys.toRedact', ''),\n ('spark.rdd.compress', 'true'),\n ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException',\n  'false'),\n ('spark.databricks.python.defaultPythonRepl', 'ipykernel'),\n ('spark.app.id', 'local-1670379098148'),\n ('spark.databricks.clusterUsageTags.clusterName',\n  \"d3bde3fb-687d-4f06-b8c0-5175027fbfe5 8d5163f7-62fd-4256-af8f-14e2eb3357a5's Personal Compute Cluster\"),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env', ''),\n ('spark.databricks.eventLog.dir', 'eventlogs'),\n ('spark.databricks.credential.scope.fs.adls.gen2.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.CredentialScopeADLSTokenProvider'),\n ('spark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled', 'false'),\n ('spark.databricks.driverNfs.pathSuffix', '.ephemeral_nfs'),\n ('spark.databricks.clusterUsageTags.clusterCreator', 'Webapp'),\n ('spark.speculation', 'false'),\n ('spark.hadoop.hive.server2.session.check.interval', '60000'),\n ('spark.sql.hive.convertCTAS', 'true'),\n ('spark.hadoop.fs.s3a.max.total.tasks', '1000'),\n ('spark.hadoop.spark.sql.parquet.output.committer.class',\n  'org.apache.spark.sql.parquet.DirectParquetOutputCommitter'),\n ('spark.databricks.tahoe.logStore.aws.class',\n  'com.databricks.tahoe.store.MultiClusterLogStore'),\n ('spark.hadoop.fs.s3a.fast.upload.default', 'true'),\n ('spark.databricks.clusterUsageTags.clusterGeneration', '0'),\n ('spark.hadoop.fs.mlflowdbfs.impl',\n  'com.databricks.mlflowdbfs.MlflowdbfsFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterUnityCatalogMode',\n  'LEGACY_SINGLE_USER_STANDARD'),\n ('spark.databricks.eventLog.listenerClassName',\n  'com.databricks.backend.daemon.driver.DBCEventLoggingListener'),\n ('spark.hadoop.fs.abfs.impl.disable.cache', 'true'),\n ('spark.speculation.multiplier', '3'),\n ('spark.storage.blockManagerTimeoutIntervalMs', '300000'),\n ('spark.databricks.clusterUsageTags.clusterAllTags',\n  '[{\"key\":\"ResourceClass\",\"value\":\"SingleNode\"},{\"key\":\"Vendor\",\"value\":\"Databricks\"},{\"key\":\"Creator\",\"value\":\"coding.moh@gmail.com\"},{\"key\":\"ClusterName\",\"value\":\"d3bde3fb-687d-4f06-b8c0-5175027fbfe5 8d5163f7-62fd-4256-af8f-14e2eb3357a5\\'s Personal Compute Cluster\"},{\"key\":\"ClusterId\",\"value\":\"1207-020733-iiri6xet\"},{\"key\":\"DatabricksEnvironment\",\"value\":\"workerenv-4401470379569556\"}]'),\n ('spark.repl.class.outputDir',\n  '/local_disk0/tmp/repl/spark-6092437230899731836-99809bbf-b25d-4a18-87db-b765dc044ed3'),\n ('spark.sparkr.use.daemon', 'false'),\n ('spark.scheduler.listenerbus.eventqueue.capacity', '20000'),\n ('spark.databricks.clusterUsageTags.driverPublicDns', '108.143.52.240'),\n ('spark.databricks.clusterUsageTags.clusterStateMessage', 'Starting Spark'),\n ('spark.hadoop.parquet.page.write-checksum.enabled', 'true'),\n ('spark.hadoop.databricks.s3commit.client.sslTrustAll', 'false'),\n ('spark.hadoop.fs.s3a.threads.max', '136'),\n ('spark.r.backendConnectionTimeout', '604800'),\n ('spark.hadoop.fs.abfss.impl',\n  'shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystemHadoop3'),\n ('spark.hadoop.hive.server2.idle.session.timeout', '900000'),\n ('spark.databricks.redactor',\n  'com.databricks.spark.util.DatabricksSparkLogRedactorProxy'),\n ('spark.executor.extraClassPath',\n  '/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*'),\n ('spark.databricks.clusterUsageTags.userId', '871826826438401'),\n ('spark.hadoop.fs.fcfs-abfs.impl.disable.cache', 'true'),\n ('spark.hadoop.parquet.page.verify-checksum.enabled', 'true'),\n ('spark.logConf', 'true'),\n ('spark.databricks.clusterUsageTags.enableJobsAutostart', 'true'),\n ('spark.hadoop.hive.server2.enable.doAs', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedSparkVersion',\n  '11.3.x-scala2.12'),\n ('spark.hadoop.parquet.filter.columnindex.enabled', 'false'),\n ('spark.hadoop.spark.driverproxy.customHeadersToProperties',\n  'X-Databricks-User-Token:spark.databricks.token,X-Databricks-Non-UC-User-Token:spark.databricks.non.uc.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name'),\n ('spark.shuffle.memoryFraction', '0.2'),\n ('spark.databricks.clusterUsageTags.driverContainerId',\n  'fffe1c2a84be4ed3b6e4e7e5c8acc9c6'),\n ('spark.hadoop.fs.dbfsartifacts.impl',\n  'com.databricks.backend.daemon.data.client.DBFSV1'),\n ('spark.hadoop.fs.cpfs-s3a.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.hadoop.fs.s3a.connection.timeout', '50000'),\n ('spark.databricks.secret.envVar.keys.toRedact', ''),\n ('spark.databricks.clusterUsageTags.cloudProvider', 'Azure'),\n ('spark.files.useFetchCache', 'false')]"]}}],"execution_count":0},{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5d6868db-bb80-41e1-a45b-042c33c5a509","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=4401470379569556#setting/sparkui/1207-020733-iiri6xet/driver-566249776327612627\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*, 4]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=4401470379569556#setting/sparkui/1207-020733-iiri6xet/driver-566249776327612627\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*, 4]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        "]}}],"execution_count":0},{"cell_type":"markdown","source":["As you can see the app name is exactly how we set it\n\nLet's create our first dataframe from a fairly small sample data set. Througout the course we'll work with a log file data set that describes user interactions with a music streaming service. The records describe events such as logging in to the site, visiting a page, listening to the next song, seeing an ad."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b277f2ed-bd3f-488e-aa19-4ad62fbe292b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["path = \"/FileStore/tables/music_log_small.json\"\n#path = data/music_log_small.json\nuser_log = spark.read.json(path)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"de2bdcbc-4b78-42d2-bc0e-011a70770778","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["user_log.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f91cd479-259d-44af-8b49-f0b597120168","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- artist: string (nullable = true)\n |-- auth: string (nullable = true)\n |-- firstName: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- itemInSession: long (nullable = true)\n |-- lastName: string (nullable = true)\n |-- length: double (nullable = true)\n |-- level: string (nullable = true)\n |-- location: string (nullable = true)\n |-- method: string (nullable = true)\n |-- page: string (nullable = true)\n |-- registration: long (nullable = true)\n |-- sessionId: long (nullable = true)\n |-- song: string (nullable = true)\n |-- status: long (nullable = true)\n |-- ts: long (nullable = true)\n |-- userAgent: string (nullable = true)\n |-- userId: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- artist: string (nullable = true)\n |-- auth: string (nullable = true)\n |-- firstName: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- itemInSession: long (nullable = true)\n |-- lastName: string (nullable = true)\n |-- length: double (nullable = true)\n |-- level: string (nullable = true)\n |-- location: string (nullable = true)\n |-- method: string (nullable = true)\n |-- page: string (nullable = true)\n |-- registration: long (nullable = true)\n |-- sessionId: long (nullable = true)\n |-- song: string (nullable = true)\n |-- status: long (nullable = true)\n |-- ts: long (nullable = true)\n |-- userAgent: string (nullable = true)\n |-- userId: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# can be slow if using whole logs\nuser_log.describe()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"88ba34a6-b68b-4b89-b76e-432aee82cb55","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[8]: DataFrame[summary: string, artist: string, auth: string, firstName: string, gender: string, itemInSession: string, lastName: string, length: string, level: string, location: string, method: string, page: string, registration: string, sessionId: string, song: string, status: string, ts: string, userAgent: string, userId: string]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[8]: DataFrame[summary: string, artist: string, auth: string, firstName: string, gender: string, itemInSession: string, lastName: string, length: string, level: string, location: string, method: string, page: string, registration: string, sessionId: string, song: string, status: string, ts: string, userAgent: string, userId: string]"]}}],"execution_count":0},{"cell_type":"code","source":["user_log.show(n=1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"89b33952-d6d1-4d13-8a25-26fb81e0f151","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n|       artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n|Showaddywaddy|Logged In|  Kenneth|     M|          112|Matthews|232.93342| paid|Charlotte-Concord...|   PUT|NextSong|1509380319284|     5132|Christmas Tears W...|   200|1513720872284|\"Mozilla/5.0 (Win...|  1046|\n+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\nonly showing top 1 row\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n|       artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n|Showaddywaddy|Logged In|  Kenneth|     M|          112|Matthews|232.93342| paid|Charlotte-Concord...|   PUT|NextSong|1509380319284|     5132|Christmas Tears W...|   200|1513720872284|\"Mozilla/5.0 (Win...|  1046|\n+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\nonly showing top 1 row\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["user_log.take(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d3ac33da-7106-4136-9d7b-c5826f3d11ba","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[10]: [Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046')]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[10]: [Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046')]"]}}],"execution_count":0},{"cell_type":"code","source":["out_path = \"/data/music_log.csv\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"43f7cdbf-a147-4418-9496-fd66a8bfd7f5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["user_log.limit(10).write.mode('overwrite').csv(out_path, header=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8aa62101-37e5-4adc-a548-5ec91fa947e6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["user_log_2 = spark.read.csv(out_path, header=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a4a2cd49-1a07-47c0-981e-1212f36412c5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["user_log_2.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4a5bdc6f-fe4d-428b-abda-8737736717e8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- artist: string (nullable = true)\n |-- auth: string (nullable = true)\n |-- firstName: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- itemInSession: string (nullable = true)\n |-- lastName: string (nullable = true)\n |-- length: string (nullable = true)\n |-- level: string (nullable = true)\n |-- location: string (nullable = true)\n |-- method: string (nullable = true)\n |-- page: string (nullable = true)\n |-- registration: string (nullable = true)\n |-- sessionId: string (nullable = true)\n |-- song: string (nullable = true)\n |-- status: string (nullable = true)\n |-- ts: string (nullable = true)\n |-- userAgent: string (nullable = true)\n |-- userId: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- artist: string (nullable = true)\n |-- auth: string (nullable = true)\n |-- firstName: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- itemInSession: string (nullable = true)\n |-- lastName: string (nullable = true)\n |-- length: string (nullable = true)\n |-- level: string (nullable = true)\n |-- location: string (nullable = true)\n |-- method: string (nullable = true)\n |-- page: string (nullable = true)\n |-- registration: string (nullable = true)\n |-- sessionId: string (nullable = true)\n |-- song: string (nullable = true)\n |-- status: string (nullable = true)\n |-- ts: string (nullable = true)\n |-- userAgent: string (nullable = true)\n |-- userId: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["user_log_2.take(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"45373576-26b1-4e82-9f86-c34ecb0ecf0c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["user_log_2.select(\"userID\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3a327a90-0c41-4305-99fa-9d26d12c074d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-2967774331460094>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0muser_log_2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"userID\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 'user_log_2' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'user_log_2' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-2967774331460094>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0muser_log_2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"userID\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 'user_log_2' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["user_log_2.take(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"90f9df96-df9c-4d73-85de-a850c5dce3e8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.8.0","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"3_data_inputs_and_outputs","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2967774331460075}},"nbformat":4,"nbformat_minor":0}
